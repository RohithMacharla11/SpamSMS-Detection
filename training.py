# -*- coding: utf-8 -*-
"""SMS SPAN DETECTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1de8SrHMsK8XwRqtzPtK6xxHoA50RTnmo
"""

import pandas as pd
import re
import nltk
import pickle
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, SpatialDropout1D
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Load dataset
file_path = "data/spam.csv"
df = pd.read_csv(file_path, encoding='latin-1')

# Display first few rows
print("First 5 Rows:")
print(df.head())

# Check basic information
print("\nDataset Info:")
print(df.info())

# Rename columns
df = df.iloc[:, :2]  # Keep only the first two columns
df.columns = ["label", "message"]  # Rename columns

#Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Check for duplicate values
print("\nDuplicate Values:", df.duplicated().sum())

# Remove duplicate rows
df = df.drop_duplicates()

print("Duplicate Values:", df.duplicated().sum())

# Display cleaned dataset info
print("Dataset Info After Cleaning:")
print(df.info())

# Convert labels to binary (optional)
df["label"] = df["label"].map({"ham": 0, "spam": 1})  # Ham = 0, Spam = 1

# Display class distribution
print("\nClass Distribution:")
print(df["label"].value_counts())

# Plot updated class distribution
plt.figure(figsize=(6, 4))
sns.countplot(x=df["label"], palette="pastel")
plt.title("Spam vs Ham Distribution (Cleaned Data)")
plt.xlabel("Message Type (0 = Ham, 1 = Spam)")
plt.ylabel("Count")
plt.show()

def preprocess_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'\W', ' ', text)  # Remove special characters
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    tokens = word_tokenize(text)  # Tokenization
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization
    return ' '.join(tokens)

df["message"] = df["message"].apply(preprocess_text)

X = df["message"]
y = df["label"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Convert text into numerical features using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
X_train_vectorized = vectorizer.fit_transform(X_train)
X_test_vectorized = vectorizer.transform(X_test)

# Save vectorizer for later use
with open("tfidf_vectorizer.pkl", "wb") as vectorizer_file:
    pickle.dump(vectorizer, vectorizer_file)

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)

# Show updated dataset info
print("Dataset Info After Preprocessing:")
print(df.info())

# Show new class distribution after balancing
print("\nUpdated Class Distribution After SMOTE:")
print(pd.Series(y_train_resampled).value_counts())

# Save processed data
import pickle
with open("processed_data.pkl", "wb") as f:
    pickle.dump((X_train_vectorized, X_test_vectorized, y_train, y_test, vectorizer), f)

print("âœ… Data Preprocessing Complete & Saved!")

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
import pickle

nb_model = MultinomialNB()
nb_model.fit(X_train_resampled, y_train_resampled)

y_pred_nb = nb_model.predict(X_test_vectorized)

print(f"âœ… NaÃ¯ve Bayes Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}")
print(f"ðŸ“Š Classification Report:\n{classification_report(y_test, y_pred_nb)}")

# Save model
with open("nb_model.pkl", "wb") as model_file:
    pickle.dump(nb_model, model_file)

print("âœ… NaÃ¯ve Bayes Model Saved!")

from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression(max_iter=500)
lr_model.fit(X_train_resampled, y_train_resampled)

y_pred_lr = lr_model.predict(X_test_vectorized)

print(f"âœ… Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}")
print(f"ðŸ“Š Classification Report:\n{classification_report(y_test, y_pred_lr)}")

# Save model
with open("lr_model.pkl", "wb") as model_file:
    pickle.dump(lr_model, model_file)

print("âœ… Logistic Regression Model Saved!")

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=100)
rf_model.fit(X_train_resampled, y_train_resampled)

y_pred_rf = rf_model.predict(X_test_vectorized)

print(f"âœ… Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}")
print(f"ðŸ“Š Classification Report:\n{classification_report(y_test, y_pred_rf)}")

# Save model
with open("rf_model.pkl", "wb") as model_file:
    pickle.dump(rf_model, model_file)

print("âœ… Random Forest Model Saved!")

from sklearn.svm import SVC

svm_model = SVC(kernel="linear", C=1.0)
svm_model.fit(X_train_resampled, y_train_resampled)

y_pred_svm = svm_model.predict(X_test_vectorized)

print(f"âœ… SVM Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}")
print(f"ðŸ“Š Classification Report:\n{classification_report(y_test, y_pred_svm)}")

# Save model
with open("svm_model.pkl", "wb") as model_file:
    pickle.dump(svm_model, model_file)

print("âœ… SVM Model Saved!")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.preprocessing import LabelEncoder

# Tokenize text
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Pad sequences
max_length = max(len(seq) for seq in X_train_seq)
X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding="post")
X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding="post")

# Encode labels
label_encoder = LabelEncoder()
y_train_enc = label_encoder.fit_transform(y_train)
y_test_enc = label_encoder.transform(y_test)

# Build LSTM Model
lstm_model = Sequential([
    Embedding(input_dim=5000, output_dim=128, input_length=max_length),
    SpatialDropout1D(0.2),
    LSTM(100),
    Dense(1, activation="sigmoid")
])

lstm_model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

# Train model
lstm_model.fit(X_train_pad, y_train_enc, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test_enc))

# Save model
lstm_model.save("lstm_model.keras")

print("âœ… LSTM Model Saved!")

from tensorflow.keras.layers import GRU

gru_model = Sequential([
    Embedding(input_dim=5000, output_dim=128, input_length=max_length),
    SpatialDropout1D(0.2),
    GRU(100),
    Dense(1, activation="sigmoid")
])

gru_model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

# Train model
gru_model.fit(X_train_pad, y_train_enc, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test_enc))

# Save model
gru_model.save("gru_model.keras")

print("âœ… GRU Model Saved!")

import pickle
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier

# Save TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)

# Impute missing values with an empty string before fitting the vectorizer
df['clean_text'] = df['clean_text'].fillna('')

X_tfidf = tfidf_vectorizer.fit_transform(df['clean_text'])

# Train Final Random Forest Model
final_rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
final_rf_model.fit(X_tfidf, df['label'])

# Save Model & Vectorizer
with open("spam_detector.pkl", "wb") as model_file:
    pickle.dump(final_rf_model, model_file)

with open("tfidf_vectorizer.pkl", "wb") as vectorizer_file:
    pickle.dump(tfidf_vectorizer, vectorizer_file)

print("âœ… Model & Vectorizer Saved Successfully!")

import pickle
import numpy as np

# Load Model & Vectorizer
with open("rf_model.pkl", "rb") as model_file:
    loaded_model = pickle.load(model_file)

with open("tfidf_vectorizer.pkl", "rb") as vectorizer_file:
    loaded_vectorizer = pickle.load(vectorizer_file)

# Function to Predict Spam/Ham
def predict_sms(text):
    text_cleaned = preprocess_text(text)  # Error occurs here
    text_tfidf = loaded_vectorizer.transform([text_cleaned])
    prediction = loaded_model.predict(text_tfidf)
    return "Spam" if prediction == 1 else "Ham"

# Example Usage
print(predict_sms("Congratulations! You won a free trip to Dubai!"))
print(predict_sms("WINNER!! As a valued network customer you have been selected to receivea ï¿½900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only"))
print(predict_sms("URGENT: Your bank account has been compromised. Verify details now!"))